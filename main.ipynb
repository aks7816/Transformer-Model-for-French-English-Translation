{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# print(sys.executable)\n",
    "\n",
    "# !{sys.executable} -m pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_LAYERS = 4\n",
    "# EMBED_DIM = 128\n",
    "# NUM_HEADS = 8\n",
    "# FF_DIM = 512\n",
    "# SRC_VOCAB_SIZE = 10000\n",
    "# TGT_VOCAB_SIZE = 10000\n",
    "# MAX_LEN = 100 \n",
    "\n",
    "# DROPOUT = 0.1\n",
    "\n",
    "NUM_LAYERS = 2\n",
    "EMBED_DIM = 64\n",
    "NUM_HEADS = 4\n",
    "FF_DIM = 126\n",
    "SRC_VOCAB_SIZE = 100\n",
    "TGT_VOCAB_SIZE = 100\n",
    "MAX_LEN = 50\n",
    "DROPOUT = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qkv: Projects input to queries (Q), keys (K), and values (V).\n",
    "# attention_scores: Compute attention weights.\n",
    "# fc_out: Merges multi-head outputs.\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module): #We are doing multi-head self attention implementation, inheriting from nn.Module\n",
    "    #It calculates relationships between tokens in the sequence.\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == embed_dim\n",
    "        \n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3) #Fully connected linear layer that maps the input embeddings to three outputs: queries (Q), keys (K), and values (V)\n",
    "        self.fc_out = nn.Linear(embed_dim, embed_dim) # a linear layer that projects the final multi-head attention output back to the original embedding dimension\n",
    "        # After attention computation, we need to merge the contributions of all heads into a single, meaningful representation\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, embed_dim = x.shape #x is the input tensor of shape [batch_size, seq_length, embed_dim]\n",
    "        qkv = self.qkv(x)  # Shape: [batch, seq_len, embed_dim * 3]\n",
    "        #the embedding values are typically transformed using a linear layer to create separate components for Query (Q), Key (K), and Value (V)\n",
    "        qkv = qkv.reshape(batch_size, seq_length, 3, self.num_heads, self.head_dim)\n",
    "        q, k, v = qkv.unbind(dim = 2)\n",
    "\n",
    "        attention_scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim) # dot product of queries and keys, followed by scaling, scales down the values to avoid excessively large scores due to the dimensionality.\n",
    "        #Transposing helps align the dimensions to correctly combine attention probabilities with the Value vectors\n",
    "        attention_probs = attention_scores.softmax(dim = 1) # transforming the scores into probabilities\n",
    "\n",
    "        out = (attention_probs @ v).transpose(1, 2).reshape(batch_size, seq_length, embed_dim)\n",
    "        return self.fc_out(out)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_dim: Dimension of the input features.\n",
    "# ff_dim: Dimension of the hidden layer in the feedforward network.\n",
    "# dropout: Dropout rate, used to prevent overfitting.\n",
    "\n",
    "class FeedForward(nn.Module): #Applies non-linear transformations to enhance token representations, more accurate and contextually rich\n",
    "\n",
    "    def __init__(self, embed_dim, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, ff_dim) #temporarily expands the dimensionality of the token embeddings, \n",
    "        self.fc2 = nn.Linear(ff_dim, embed_dim) #allows the model to learn more complex patterns for each token before projecting them back to the original dimension\n",
    "        self.dropout = nn.Dropout(dropout) #randomly zeroing out some activations during training, so the model does not rely on specific features\n",
    "        self.relu = nn.ReLU() #It introduces non-linearity and helps the model learn complex patterns.\n",
    "# Processes the attention output.\n",
    "# Adds non-linearity and depth to the model.\n",
    "\n",
    "    def forward(self, x): #defines how the input data flows through the layers of the network.\n",
    "        return self.fc2(self.dropout(self.relu(self.fc1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module): #Combines attention and feed-forward layers with normalization and residual connections.\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ff = FeedForward(embed_dim, ff_dim, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout) #randomly zero out some activations during training\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_out = self.attention(x)\n",
    "        x = self.norm1(x + attention_out) #normalizes the output after self-attention\n",
    "        ff_out = self.ff(x)\n",
    "        return self.norm2(x + ff_out) #normalizes the output after the feed-forward layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_layers: The number of layers in the encoder.\n",
    "# embed_dim: Dimension of the embeddings.\n",
    "# num_heads: Number of attention heads in multi-head self-attention.\n",
    "# ff_dim: Dimension of the feed-forward layer.\n",
    "# vocab_size: Size of the vocabulary (number of unique tokens in the dataset).\n",
    "# max_len: Maximum sequence length for positional encoding.\n",
    "# dropout: Dropout rate for regularization, default is 0.1.\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module): #Encodes the input sequence into rich contextualized representations\n",
    "    def __init__(self, num_layers, embed_dim, num_heads, ff_dim, vocab_size, max_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim) #Turn input token indices to embeddings vectors of embed_dim dimension\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_len, embed_dim)) #Positional encoding allows the model to learn the position of tokens in the sequence\n",
    "        self.layers = nn.ModuleList([     #creates multiple instances of the TransformerEncoderLayer with the specified dimensions for attention heads, feed-forward dimensions, and dropout.\n",
    "            TransformerEncoderLayer(embed_dim, num_heads, ff_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_length = x.size(1)\n",
    "        x = self.embedding(x) + self.positional_encoding[:, :seq_length, :]\n",
    "        for layer in self.layers: #More layers lead to richer contextual representations, allowing the model to capture complex relationships between tokens.\n",
    "            x = layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module): #complete transformer that combines the encoder and output projection.\n",
    "    def __init__(self, num_layers, embed_dim, num_heads, ff_dim, src_vocab_size, tgt_vocab_size, max_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim, src_vocab_size, max_len, dropout)\n",
    "        self.fc_out = nn.Linear(embed_dim, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src):\n",
    "        encoded = self.encoder(src)\n",
    "        logits = self.fc_out(encoded)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Vocab Size: 6228625\n",
      "Target Vocab Size: 6079061\n",
      "Sample from dataset: (tensor([ 1,  2,  3,  1,  4,  3,  5,  6,  7,  3,  8,  9, 10, 11,  3, 12,  3, 13,\n",
      "         3, 14,  3, 15,  3, 16,  3, 17, 18,  3, 19,  3, 20, 21, 22, 23, 24, 11,\n",
      "        25,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]), tensor([ 1,  2,  3,  4,  5,  6,  1,  2,  3,  7,  8,  6,  9, 10,  6, 11, 12, 13,\n",
      "        14, 15, 16,  6, 17,  6, 18,  6, 19,  6, 20,  6, 21,  6, 22, 23,  6, 24,\n",
      "         6, 25, 26, 27, 14, 28,  0,  0,  0,  0,  0,  0,  0,  0]))\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from the CSV file\n",
    "csv_file_name = \"en-fr 2.csv\"  # Replace with your CSV file path\n",
    "data = pd.read_csv(csv_file_name)\n",
    "\n",
    "# Extract source (English) and target (French) sentences\n",
    "src_sentences = data.iloc[:, 0].fillna(\"\").astype(str).tolist()  # English sentences\n",
    "tgt_sentences = data.iloc[:, 1].fillna(\"\").astype(str).tolist()  # French translations\n",
    "\n",
    "# Create dynamic vocabularies\n",
    "src_vocab = defaultdict(lambda: len(src_vocab))  # Source vocabulary\n",
    "tgt_vocab = defaultdict(lambda: len(tgt_vocab))  # Target vocabulary\n",
    "\n",
    "src_vocab[\"<UNK>\"] = 1  # Default index for unknown tokens in source vocabulary\n",
    "tgt_vocab[\"<UNK>\"] = 1 \n",
    "\n",
    "# Build vocabularies from the dataset\n",
    "for sentence in src_sentences:\n",
    "    for token in sentence.split():\n",
    "        _ = src_vocab[token]  # Add token to source vocab\n",
    "\n",
    "for sentence in tgt_sentences:\n",
    "    for token in sentence.split():\n",
    "        _ = tgt_vocab[token]  # Add token to target vocab\n",
    "\n",
    "# Convert vocabularies to regular dictionaries\n",
    "src_vocab = dict(src_vocab)\n",
    "tgt_vocab = dict(tgt_vocab)\n",
    "\n",
    "# Define the dataset class\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab, max_len=50):\n",
    "        self.src_sentences = src_sentences\n",
    "        self.tgt_sentences = tgt_sentences\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_sentences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        src_sentence = self.src_sentences[index]\n",
    "        tgt_sentence = self.tgt_sentences[index]\n",
    "\n",
    "        # Tokenize sentences and pad with zeros to max_len\n",
    "        src_tokens = [self.src_vocab.get(token, 1) for token in src_sentence.split()]  # Default to 1 for unknown tokens\n",
    "        tgt_tokens = [self.tgt_vocab.get(token, 1) for token in tgt_sentence.split()]  # Default to 1 for unknown tokens\n",
    "\n",
    "        # Pad sequences to max_len\n",
    "        src_tokens = src_tokens + [0] * (self.max_len - len(src_tokens))\n",
    "        tgt_tokens = tgt_tokens + [0] * (self.max_len - len(tgt_tokens))\n",
    "\n",
    "        return torch.tensor(src_tokens[:self.max_len]), torch.tensor(tgt_tokens[:self.max_len])\n",
    "\n",
    "# Create the dataset\n",
    "dataset = SimpleDataset(src_sentences, tgt_sentences, src_vocab, tgt_vocab)\n",
    "\n",
    "# Check dataset and vocab sizes\n",
    "print(f\"Source Vocab Size: {len(src_vocab)}\")\n",
    "print(f\"Target Vocab Size: {len(tgt_vocab)}\")\n",
    "print(f\"Sample from dataset: {dataset[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Training on batches allows the model to process data more efficiently.This helps in parallelizing computations, reducing training time, and managing memory usage.\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 16 #DataLoader will return batches of 2 samples at a time\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Now in forward pass\n",
      "backward pass\n",
      "Epoch 1/1, Loss: 15.7326\n"
     ]
    }
   ],
   "source": [
    "# Subset the dataset to use only the first 100,000 samples per epoch\n",
    "subset_size = 1000  # Use only the first 100,000 samples per epoch\n",
    "subset = torch.utils.data.Subset(dataset, list(range(subset_size)))\n",
    "train_dataloader = DataLoader(subset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Dynamically adjust vocab sizes\n",
    "SRC_VOCAB_SIZE = max(SRC_VOCAB_SIZE, max([src.max().item() for src, _ in dataset]) + 1)\n",
    "TGT_VOCAB_SIZE = max(TGT_VOCAB_SIZE, max([tgt.max().item() for _, tgt in dataset]) + 1)\n",
    "\n",
    "# Instantiate model\n",
    "model = Transformer(NUM_LAYERS, EMBED_DIM, NUM_HEADS, FF_DIM, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, MAX_LEN, DROPOUT)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignoring padding tokens (0), computes the cross-entropy between predicted logits and actual target indices\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    print(\"In epoch\")\n",
    "    for src, tgt in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        print(\"Now in forward pass\")\n",
    "        # Forward pass\n",
    "        output = model(src)  # generates predictions\n",
    "        loss = criterion(output.view(-1, TGT_VOCAB_SIZE), tgt.view(-1))  # flattened output is then compared with the true tokens to calculate the loss\n",
    "        print(\"backward pass\")\n",
    "        # Backward pass\n",
    "        loss.backward()  # computes the gradients of the model parameters with respect to the loss\n",
    "        optimizer.step()  # updates the model parameters based on the gradients calculated.\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"transformer_model.pth\")\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 15.6962\n"
     ]
    }
   ],
   "source": [
    "# Validation loop (separate chunk)\n",
    "validation_size = 500  # Use the next 500 samples for validation\n",
    "validation_subset = torch.utils.data.Subset(dataset, list(range(subset_size, subset_size + validation_size)))\n",
    "validation_dataloader = DataLoader(validation_subset, batch_size=16, shuffle=False)\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "total_val_loss = 0\n",
    "with torch.no_grad():  # Disable gradient calculation for validation\n",
    "    for src, tgt in validation_dataloader:\n",
    "        output = model(src)  # generates predictions\n",
    "        val_loss = criterion(output.view(-1, TGT_VOCAB_SIZE), tgt.view(-1))  # calculate validation loss\n",
    "        total_val_loss += val_loss.item()\n",
    "\n",
    "avg_val_loss = total_val_loss / len(validation_dataloader)\n",
    "print(f\"Validation Loss: {avg_val_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu kernel",
   "language": "python",
   "name": "gpu_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
